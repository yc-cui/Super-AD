


\section{Methodology}\label{Methodology}






\subsection{Unified Perspective for Self-supervised HAD}
% \vspace{-1mm}

In the context of self-supervised HAD, we address the IMP by proposing a unified framework that encompasses perturbation, reconstruction, and regularization. Given a hyperspectral image $\mathbf{X}\in \mathbb{R}^{h\times w \times c}$ containing anomalous pixels, we utilize a neural network $\mathcal{F}$ parameterized by $\theta$ to reconstruct the image. We summarize the optimization process of a theoretically well-performing neural network using the following unified formulation:
% \vspace{-3mm}
\begin{equation}
    \hat{\theta}  = \arg \min_{\theta}\mathcal{L} \left(\mathcal{F}\left(\mathcal{P}\left(\mathrm{\mathbf{X}}\right), \hat{\mathrm{\mathbf{M}}};\theta \right) , \mathrm{\mathbf{X}}\right) + \lambda \mathcal{R}(\hat{\mathrm{\mathbf{M}}}),
\end{equation}
where $\mathcal{L}(\cdot,\cdot)$ denotes the reconstruction loss (commonly $l_1$ or $l_2$), and $\hat{\mathbf{M}}$ is the estimated anomaly map from the previous iteration. The critical components of gaining insights into IMP include the data perturbation operation $\mathcal{P}(\cdot)$, the guided reconstruction function $\mathcal{F}(\cdot,\cdot;\theta)$, and the regularization term $\mathcal{R}(\cdot)$. $\lambda$ balances the contribution of the regularization. In the following, we will introduce the design of each component in detail. We argue that the recently proposed HAD methods related to the IMP can be incorporated into our framework.


\subsubsection{Perturbation}

The data perturbation operation $\mathcal{P}(\cdot)$ is designed to perturb the spectral information. Applying perturbations to obscure the information of anomalous spectra before they can influence the network's reconstruction process is a straightforward strategy to mitigate the IMP. The random masking strategy in SMCNet~\cite{SMCNet} and AETNet~\cite{AETNet}, the use of noise in AutoAD~\cite{AutoAD} and BSDM~\cite{BSDM}, \textit{etc.}, are specific perturbation instances. The blind spot network series, BS3LNet~\cite{BS3LNet}, BockNet~\cite{BockNet}, PDBSNet~\cite{PDBSNet}, DirectNet~\cite{DirectNet}, \textit{etc.}, can actually be considered a form of perturbation, which can be viewed as applying a mask to the central pixel during convolution.


\subsubsection{Reconstruction}

The reconstruction function $\mathcal{F}(\cdot,\cdot;\theta)$ leverages the estimated anomaly map $\hat{\mathbf{M}} \in \mathbb{R}^{h \times w}$ from the previous iteration to guide the forward process of the network. Given the premise that anomalous spectra are difficult to reconstruct, the reconstruction error from the previous iteration can be treated as a confidence measure for the anomaly map. By designing a suitable weighting function for $\hat{\mathrm{\mathbf{M}}}$, we can enhance the error associated with anomalous spectra while diminishing the error related to the background, leading to a more accurate anomaly map in subsequent iterations and thus mitigating the IMP. For instance, BiGSeT~\cite{BiGSeT} and MSNet~\cite{MSNet} utilized the dot product to modify the reconstruction results. AutoAD~\cite{AutoAD}, DeepLR~\cite{DeepLR}, and S2DWMTrans~\cite{S2DWMTrans} employed adaptive weights to alter the gradients during the backpropagation.



\subsubsection{Regularization}

This term imposes constraints on the estimated anomaly map to prevent the IMP. The weight coefficient $\lambda$ balances the contributions of the reconstruction loss and the regularization term. In the optimization process of the neural network, this term is typically formulated as a loss function that imposes additional constraints on the anomaly map. For instance, BiGSeT~\cite{BiGSeT} and MSNet~\cite{MSNet} applied the second-order Laplacian of Gaussian (LoG) operator to suppress anomalies. DeepLR~\cite{DeepLR} and RSAAE~\cite{RSAAE} applied a low-rank regularized loss to constrain the network to approximate the low-rank background. However, a common challenge in existing methods is the difficulty in determining the balance coefficient $\lambda$ between reconstruction and regularization.

Although each part presents various methods, their limited consideration of the reconstruction process from a holistic perspective of network optimization results in constrained performance. In this paper, we meticulously designed these three key aspects, and experiments prove that our approach can achieve optimal results (see Fig.~\ref{fig:net}).



\subsection{Design of the Perturbation Operation $\mathcal{P}$}


Masking~\cite{SMCNet,AETNet,BS3LNet,BockNet,PDBSNet,DirectNet} and noise~\cite{AutoAD,BSDM} cannot ensure the total elimination of anomalous spectra before sent into the network. To this end, we propose a new perturbation strategy, \textit{i.e.,} superpixel pooling and unpooling (dubbed as SPP). Specifically, we first use Simple Linear Iterative Clustering (SLIC)~\cite{SLIC} to segment the hyperspectral image into superpixels, and then apply average pooling to each region block to retain the average feature information. Since anomalies occupy a small proportion, they are easily wrapped in pixel blocks surrounded by the background. Due to the average pooling strategy, the block information will contain mostly background spectra while ignoring the anomalous spectra, which prevents the anomalous spectra from being reconstructed, thereby mitigating the IMP. Meanwhile, for the extracted all blocks, we use the self-attention mechanism~\cite{vaswani2017attention,ViT} to perform spectral reconstruction, learning the relationship between the blocks. Finally, all blocks will perform uppooling to revert to original size. Compared to masking~\cite{SMCNet,AETNet,BS3LNet,BockNet,PDBSNet,DirectNet} and noise~\cite{AutoAD,BSDM} strategies, SPP effectively encapsulates anomalous pixels within background-dominated blocks, thereby preventing their influence on the reconstruction process.



Formally, given a hyperspectral image $\mathrm{\mathbf{X}} \in \mathbb{R}^{h\times w \times c}$, $\operatorname{SPP}(\mathrm{\mathbf{X}})$ can be described as follows. Firstly, obtaining a series of superpixel blocks using the SLIC~\cite{SLIC} algorithm,
\begin{equation}
    \mathcal{S}=\operatorname{SLIC}(\mathrm{\mathbf{X}}),
\end{equation}
where $\mathcal{S}=\{S_1, S_2, \cdots, S_m\}$, $S_i$ represents the $i$-th superpixel. Then, we apply average pooling to each superpixel to obtain the feature vectors $\mathcal{V} = \{v_1, v_2, \cdots, v_m\}$. The pooling process can be expressed as,
\begin{equation}
    v_i=\frac{1}{|S_i|}\sum_{p\in S_i}\mathrm{\mathbf{F}}_p,
\end{equation}
where $\mathrm{\mathbf{F}}_p$ denotes the feature vector of pixel $p$ used in superpixel pooling and $\left|\cdot\right|$ is the cadinality of set (\# of pixels). After forward the self-attention~\cite{vaswani2017attention,ViT}, the feature vector will be restored to its original shape through uppooling,
\begin{equation}
    \mathrm{\mathbf{U}}(x,y)=\sum_{v_i \in \mathcal{V}} v_i \cdot 1_{p_{xy} \in S_i},
\end{equation}
where $\mathrm{\mathbf{U}}\in \mathbb{R}^{h\times w \times c}$ is the uppooled feature. $1_{p_{xy} \in S_i}$ is an indicator function that is $1$ if $p$ in $(x, y)$ belongs to $S_i$, and $0$ otherwise.






\subsection{Design of the Reconstruction Function $\mathcal{F}$}


Commonly, existing designs directly use the estimated anomaly map as a weight~\cite{AutoAD,BiGSeT,MSNet,DeepLR,S2DWMTrans}, which still allow anomalous pixels to affect the reconstruction process. In contrast, we propose a novel guided reconstruction mechanism termed error-adaptive convolution (dubbed as AdaConv), which maximizes the non-utilization of anomalies. AdaConv performs dynamic convolution only on pixels that are most likely to be non-anomalous based on anomaly probability from the previous iteration. 

Specifically, given a coordinate $(x,y)$, we get the indices of all elements of a candidate window of size $n\times n$, 
\begin{equation}
    \begin{split}
    \mathcal{N}(x,y) = \{(i,j) \mid & i \in [x - \frac{n-1}{2}, x + \frac{n-1}{2}], \\
    & j \in [y - \frac{n-1}{2}, y + \frac{n-1}{2}]\}.
    \end{split}
    \end{equation}
    \vspace{-3mm}


For the estimation of the anomaly map obtained in the previous iteration, we sort the probabilities (or errors) ascendingly, and take the indices corresponding to the smallest top $k^2$ elements, where $k^2$ is the number of trainable parameters in the convolution kernel and $k \leq n$,
\begin{equation}
    \mathcal{D}(x,y)=\operatorname{argsort} (\hat{\mathrm{\mathbf{M}}}_{\mathcal{N}(x,y)})[\,:k^2].
\end{equation}
\vspace{-3mm}

Finally, convolve the feature map with elements taken from the corresponding indices in $\mathcal{D}(x,y)$,
\begin{equation}
    \begin{aligned}
    \mathrm{\mathbf{F}}^{\prime}(x,y)&=\mathrm{\mathbf{F}}_{\mathcal{D}(x,y)}  \ast \mathrm{\mathbf{K}}  \\
    &= \sum_{i=1}^{k} \sum_{j=1}^{k}  \mathrm{\mathbf{F}}(d_i,d_j)   \cdot  \mathrm{\mathbf{K}}(i,j),
\end{aligned}
\end{equation}
where $ \mathrm{\mathbf{F}}^{\prime}$ is the feature obtained by AdaConv, and $\mathrm{\mathbf{K}}$ is the trainable kernel with size of $k \times k$. The uppooled features are reconstructed to the original image by performing a dot product with features extracted using AdaConv,
\begin{equation}
    \hat{\mathrm{\mathbf{X}}}^t=\mathrm{\mathbf{U}} \odot \mathrm{\mathbf{F}}^{\prime}.
\end{equation}
where $t$ represents the current iteration. $l_2$-norm is employed to calculate the anomaly score of pixel $p$, 
\begin{equation}
    \hat{\mathrm{\mathbf{M}}}_p^{t}=\left\|\hat{\mathrm{\mathbf{X}}}_p^{t}-\mathrm{\mathbf{X}}_p\right\|_{2},
\end{equation}
and the estimated detection map is used to guide the reconstruction process in the next iteration,
\begin{equation}
    \hat{\mathrm{\mathbf{X}}}^{t+1}=\mathcal{F}( \operatorname{SPP}(\mathrm{\mathbf{X}}), \hat{\mathrm{\mathbf{M}}}^{t}; \theta ).
\end{equation}
\vspace{-3mm}





\subsection{Design of the Regularization Term $\mathcal{R}$}

The regularization term imposes constraints on the anomaly map during the backward propagation of errors. However, determining the balance coefficient between the reconstruction and regularization terms remain challenging. We propose Online Background Pixel Mining (dubbed as OBPM) loss, which simultaneously achieves more efficient reconstruction and provides stronger constraints on anomalies. OBPM incorporates two key strategies: (1) For the reconstruction of the background, the more difficult the background is to reconstruct, the larger the gradient will be contributed. Gradient will be scaled exponentially with the reconstruction error. (2) For the regularization of anomalies, we enforce the disregard of gradients generated by potential anomalies. The two aspects ensure that the model focuses on reconstructing more complex background while avoiding the influence of anomalies that could distort the training process.

\subsubsection{Reconstructing Background}

Given the absolute background reconstruction error $x$, we desire that its backpropagation yields an exponentially scaled gradient,
\begin{equation}
    g(x) = e^{\beta x} + \alpha,
\end{equation}
here, the rate of exponential growth is determined by $\beta$, whereas $\alpha$ sets the minimum gradient. Thus, the reconstruction loss can be formulated as,
\begin{equation}
    l(x) = e^{\beta x} / {\beta} + \alpha x.
\end{equation}

\subsubsection{Regularizing Anomaly} 

The ideal solution is to not allow anomalies to contribute any gradients, \textit{i.e.}, discarding potential anomalies. Specifically, for superpixel $S_i$, the reconstruction error $\mathbf{e_i}$ will be firstly sorted ascendingly, 
\begin{equation}
\mathbf{e_i}^{\prime} = \operatorname{sort}(\mathbf{e_i}) = [e_1, e_2, \ldots, e_{|\mathcal{S}_i|}],
\end{equation}
where $e_1 \leq e_2 \leq \ldots \leq e_{|\mathcal{S}_i|}$. Since the basic assumption is that the anomalies has significantly larger errors than the background, we set the index with the largest error change as the boundary,
\begin{equation}
    q = \arg\max_{j} \{e_{j+1} - e_j \}, j = 1, 2, \ldots, |\mathcal{S}_i|-1,
\end{equation}
where $e_{j+1} - e_j$ represents the first order difference in sorted error, reflecting the magnitude of the error change. $q$ is the index where the error changes the most. Any error greater than $\mathbf{e_i}^{\prime}[q]$ will be ignored. Note that this will cause some background errors to be ignored, but since we provide exponential gradients, the remaining background can still provide enough gradients for network optimization. 

Combining the reconstruction loss, the OBPM loss of an error $x$ which belongs to $S_i$ is expressed as follows,
\begin{equation}
    \operatorname{OBPM}(x_{\in S_i})=
    \begin{cases}
        e^{\beta x} / \beta + \alpha x, & \text{ if } x \leq \mathbf{e_i}^{\prime}[q] \\
       0, & \text{ otherwise} .
      \end{cases}
\end{equation}

